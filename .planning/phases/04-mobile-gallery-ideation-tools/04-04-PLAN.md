---
phase: 04-mobile-gallery-ideation-tools
plan: 04
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - portfolio/src/components/VoiceRecorder.astro
  - portfolio/src/scripts/voice-recorder.ts
  - portfolio/src/scripts/whisper-client.ts
  - portfolio/src/pages/api/transcribe.ts
  - portfolio/src/pages/mobile/[slug].astro
  - portfolio/src/components/VoiceNoteList.astro
  - portfolio/package.json
autonomous: true

must_haves:
  truths:
    - "User can tap a record button to start capturing audio from their phone microphone"
    - "User taps the same button again to stop recording"
    - "Recording auto-stops after 5 minutes as a safety limit"
    - "Audio is saved as a Blob in IndexedDB voiceNotes store"
    - "After recording stops, audio is sent to server-side Whisper API for transcription"
    - "API key never exposed to client — transcription goes through server endpoint /api/transcribe"
    - "Transcript is shown for user to review and optionally edit before saving"
    - "Previous voice notes for this atom are listed below the recorder"
    - "Voice notes are playable (audio playback from IndexedDB Blob)"
    - "MediaRecorder MIME type detection supports iOS Safari (webm;codecs=opus fallback chain)"
    - "Recording works offline (audio saved, transcription queued for when online)"
    - "Visual feedback during recording (pulsing indicator, recording timer)"
  artifacts:
    - path: "portfolio/src/components/VoiceRecorder.astro"
      provides: "Record/stop button UI with recording indicator and timer"
      contains: "record"
    - path: "portfolio/src/scripts/voice-recorder.ts"
      provides: "MediaRecorder API wrapper with MIME type detection and Blob handling"
      contains: "MediaRecorder"
    - path: "portfolio/src/scripts/whisper-client.ts"
      provides: "Client-side Whisper API communication via /api/transcribe endpoint"
      contains: "transcribe"
    - path: "portfolio/src/pages/api/transcribe.ts"
      provides: "Server endpoint that calls Whisper API with API key (never exposed to client)"
      contains: "whisper"
    - path: "portfolio/src/components/VoiceNoteList.astro"
      provides: "List of previously recorded voice notes with playback and transcript display"
      contains: "voiceNotes"
  key_links:
    - from: "portfolio/src/components/VoiceRecorder.astro"
      to: "portfolio/src/scripts/voice-recorder.ts"
      via: "import { startRecording, stopRecording }"
      pattern: "startRecording"
    - from: "portfolio/src/scripts/voice-recorder.ts"
      to: "portfolio/src/scripts/whisper-client.ts"
      via: "import { transcribeAudio }"
      pattern: "transcribeAudio"
    - from: "portfolio/src/scripts/whisper-client.ts"
      to: "portfolio/src/pages/api/transcribe.ts"
      via: "fetch('/api/transcribe')"
      pattern: "api/transcribe"
    - from: "portfolio/src/components/VoiceRecorder.astro"
      to: "portfolio/src/scripts/db.ts"
      via: "import { saveVoiceNote, getVoiceNotesForAtom }"
      pattern: "saveVoiceNote"
---

<objective>
Create the voice note recording and transcription feature for the mobile PWA. After this plan, users can tap a record button on any atom's detail page, speak their idea, stop recording, and have the audio automatically transcribed via Whisper API. The transcript appears for review/editing before saving. Previously recorded voice notes are listed with playback capability. Audio recordings are stored in IndexedDB and associated with their atom.

Purpose: Voice notes (IDEA-01, IDEA-02) are the core ideation capture tool for commute/mobile use. Speaking is faster than typing on a phone. Without voice capture, mobile ideation is limited to slow text entry. Whisper transcription converts ephemeral voice into searchable, editable text.
Output: VoiceRecorder component, voice-recorder.ts (MediaRecorder wrapper), whisper-client.ts (API communication), server transcription endpoint, VoiceNoteList component.
</objective>

<execution_context>
@/home/pavel/.claude/get-shit-done/workflows/execute-plan.md
@/home/pavel/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-mobile-gallery-ideation-tools/04-CONTEXT.md
@.planning/phases/04-mobile-gallery-ideation-tools/04-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install OpenAI SDK and create server-side transcription endpoint</name>
  <files>
    portfolio/package.json
    portfolio/src/pages/api/transcribe.ts
    portfolio/astro.config.mjs
  </files>
  <action>
Install the OpenAI SDK and create the server endpoint for Whisper transcription. The API key must never be exposed to the client — all transcription goes through this server endpoint.

**Install OpenAI SDK:**
```bash
cd portfolio && npm install openai
```

**Important: Astro SSR requirement**

The /api/transcribe endpoint needs server-side rendering (SSR). Currently the portfolio uses static site generation (SSG). We need to handle this carefully. The API route needs to run on a server. Two approaches:

**Option A (recommended): Hybrid rendering**
If Astro 5.x supports hybrid rendering (some pages static, some server), configure the API route as server-rendered while keeping the rest static. Check Astro docs for `export const prerender = false` in API routes.

**Option B: Separate Express server**
If hybrid rendering is not feasible, create a standalone Node.js server file that handles /api/transcribe outside of Astro. This keeps the portfolio fully static.

**For Option A (preferred), update portfolio/astro.config.mjs:**

Add output: 'hybrid' or check if the API route works with SSG mode by adding `export const prerender = false`:

```javascript
// In astro.config.mjs, add:
output: 'server', // or 'hybrid' if supported
```

However, changing to server mode affects the entire portfolio. A cleaner approach: **create a standalone transcription server** that runs separately.

**For the cleanest architecture, create a standalone server. Create portfolio/src/pages/api/transcribe.ts as a reference, but also create a standalone server script:**

**Create portfolio/scripts/transcribe-server.js:**

```javascript
/**
 * Standalone Whisper transcription server.
 * Runs alongside the portfolio dev/preview server.
 * Listens on port 3001 (configurable via TRANSCRIBE_PORT env var).
 *
 * Usage:
 *   OPENAI_API_KEY=sk-xxx node portfolio/scripts/transcribe-server.js
 *
 * The client-side code sends audio to this endpoint.
 */
import { createServer } from 'http';
import { OpenAI } from 'openai';
import { Readable } from 'stream';

const PORT = process.env.TRANSCRIBE_PORT || 3001;
const API_KEY = process.env.OPENAI_API_KEY;

if (!API_KEY) {
  console.error('[transcribe-server] OPENAI_API_KEY environment variable required');
  process.exit(1);
}

const openai = new OpenAI({ apiKey: API_KEY });

const server = createServer(async (req, res) => {
  // CORS headers for cross-origin requests from portfolio
  res.setHeader('Access-Control-Allow-Origin', '*');
  res.setHeader('Access-Control-Allow-Methods', 'POST, OPTIONS');
  res.setHeader('Access-Control-Allow-Headers', 'Content-Type');

  if (req.method === 'OPTIONS') {
    res.writeHead(200);
    res.end();
    return;
  }

  if (req.method !== 'POST' || req.url !== '/api/transcribe') {
    res.writeHead(404, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify({ error: 'Not found' }));
    return;
  }

  try {
    // Read request body as buffer
    const chunks = [];
    for await (const chunk of req) {
      chunks.push(chunk);
    }
    const body = Buffer.concat(chunks);

    // Parse multipart form data manually (simple implementation)
    // The client sends a FormData with 'audio' field
    const contentType = req.headers['content-type'] || '';

    if (!contentType.includes('multipart/form-data')) {
      res.writeHead(400, { 'Content-Type': 'application/json' });
      res.end(JSON.stringify({ error: 'Expected multipart/form-data' }));
      return;
    }

    // Extract boundary
    const boundaryMatch = contentType.match(/boundary=(.+)/);
    if (!boundaryMatch) {
      res.writeHead(400, { 'Content-Type': 'application/json' });
      res.end(JSON.stringify({ error: 'Missing boundary' }));
      return;
    }

    const boundary = boundaryMatch[1];
    const parts = parseMultipart(body, boundary);
    const audioPart = parts.find(p => p.name === 'audio');

    if (!audioPart) {
      res.writeHead(400, { 'Content-Type': 'application/json' });
      res.end(JSON.stringify({ error: 'No audio file in request' }));
      return;
    }

    // Create a File-like object for the OpenAI SDK
    const audioFile = new File(
      [audioPart.data],
      audioPart.filename || 'voice-note.webm',
      { type: audioPart.contentType || 'audio/webm' }
    );

    console.log(`[transcribe-server] Transcribing ${audioFile.name} (${audioFile.size} bytes)`);

    const transcription = await openai.audio.transcriptions.create({
      file: audioFile,
      model: 'whisper-1',
      response_format: 'text'
    });

    console.log(`[transcribe-server] Transcription: "${transcription.substring(0, 80)}..."`);

    res.writeHead(200, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify({ text: transcription }));

  } catch (error) {
    console.error('[transcribe-server] Error:', error.message);

    const statusCode = error.status || 500;
    res.writeHead(statusCode, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify({
      error: 'Transcription failed',
      message: error.message
    }));
  }
});

/**
 * Simple multipart form data parser.
 */
function parseMultipart(body, boundary) {
  const parts = [];
  const boundaryBuffer = Buffer.from(`--${boundary}`);
  const endBoundaryBuffer = Buffer.from(`--${boundary}--`);

  let start = body.indexOf(boundaryBuffer) + boundaryBuffer.length + 2; // Skip \r\n

  while (start < body.length) {
    const nextBoundary = body.indexOf(boundaryBuffer, start);
    if (nextBoundary === -1) break;

    const partData = body.slice(start, nextBoundary - 2); // Trim trailing \r\n
    const headerEnd = partData.indexOf('\r\n\r\n');
    if (headerEnd === -1) break;

    const headers = partData.slice(0, headerEnd).toString();
    const data = partData.slice(headerEnd + 4);

    const nameMatch = headers.match(/name="([^"]+)"/);
    const filenameMatch = headers.match(/filename="([^"]+)"/);
    const contentTypeMatch = headers.match(/Content-Type:\s*(.+)/i);

    if (nameMatch) {
      parts.push({
        name: nameMatch[1],
        filename: filenameMatch ? filenameMatch[1] : null,
        contentType: contentTypeMatch ? contentTypeMatch[1].trim() : null,
        data
      });
    }

    start = nextBoundary + boundaryBuffer.length + 2;
  }

  return parts;
}

server.listen(PORT, () => {
  console.log(`[transcribe-server] Listening on http://localhost:${PORT}/api/transcribe`);
  console.log('[transcribe-server] Ready for voice note transcription');
});
```

**Update portfolio/package.json scripts:**
Add a script to start the transcription server:
```json
"transcribe": "node scripts/transcribe-server.js"
```

**Environment variable:** The user needs to set `OPENAI_API_KEY` in their environment. Document this in a comment in the server file. The API key is NEVER sent to the client.
  </action>
  <verify>
1. `cd portfolio && npm ls openai` shows installed version
2. portfolio/scripts/transcribe-server.js exists
3. `OPENAI_API_KEY=test node portfolio/scripts/transcribe-server.js &` starts without crash (will fail on actual transcription without real key, but should start)
4. `curl -X OPTIONS http://localhost:3001/api/transcribe` returns 200 (CORS preflight)
5. portfolio/package.json has "transcribe" script
6. Kill the test server after verification
  </verify>
  <done>
OpenAI SDK installed. Standalone transcription server created that handles POST /api/transcribe with multipart form data containing audio. Server reads OPENAI_API_KEY from environment, calls Whisper API (whisper-1 model), returns transcript as JSON. CORS headers allow cross-origin requests from portfolio. API key never exposed to client. Package.json script added for easy startup.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create voice recorder engine and Whisper client</name>
  <files>
    portfolio/src/scripts/voice-recorder.ts
    portfolio/src/scripts/whisper-client.ts
  </files>
  <action>
Create the client-side recording engine and transcription client.

**Create portfolio/src/scripts/voice-recorder.ts:**

```typescript
/**
 * Voice recorder engine using MediaRecorder API.
 * Handles MIME type detection for iOS/Android compatibility,
 * recording lifecycle, and audio Blob generation.
 */

export interface RecordingState {
  isRecording: boolean;
  duration: number;  // seconds
  error: string | null;
}

export type RecordingCallback = (state: RecordingState) => void;

let mediaRecorder: MediaRecorder | null = null;
let chunks: Blob[] = [];
let startTime = 0;
let timerInterval: ReturnType<typeof setInterval> | null = null;
let recordingCallback: RecordingCallback | null = null;

const MAX_DURATION_MS = 5 * 60 * 1000; // 5 minutes safety limit

/**
 * Detect the best supported audio MIME type.
 * iOS Safari requires specific MIME types; Android Chrome is more flexible.
 * Returns the first supported type from the preference list.
 */
export function detectMimeType(): string | null {
  const candidates = [
    'audio/webm;codecs=opus',
    'audio/webm',
    'audio/mp4',
    'audio/ogg;codecs=opus',
    'audio/wav'
  ];

  for (const mime of candidates) {
    if (MediaRecorder.isTypeSupported(mime)) {
      return mime;
    }
  }

  return null;
}

/**
 * Check if voice recording is available on this device.
 */
export function isRecordingAvailable(): boolean {
  return !!(
    navigator.mediaDevices &&
    navigator.mediaDevices.getUserMedia &&
    typeof MediaRecorder !== 'undefined' &&
    detectMimeType()
  );
}

/**
 * Start recording audio from the device microphone.
 * @param onStateChange - Callback invoked with recording state updates (duration, errors)
 * @returns Promise that resolves when recording starts
 */
export async function startRecording(onStateChange: RecordingCallback): Promise<void> {
  if (mediaRecorder && mediaRecorder.state === 'recording') {
    throw new Error('Already recording');
  }

  recordingCallback = onStateChange;

  // Request microphone access
  const stream = await navigator.mediaDevices.getUserMedia({
    audio: {
      echoCancellation: true,
      noiseSuppression: true,
      sampleRate: 44100
    }
  });

  // Detect supported MIME type
  const mimeType = detectMimeType();
  if (!mimeType) {
    stream.getTracks().forEach(t => t.stop());
    throw new Error('No supported audio format on this device');
  }

  // Create recorder
  mediaRecorder = new MediaRecorder(stream, { mimeType });
  chunks = [];

  mediaRecorder.ondataavailable = (e) => {
    if (e.data.size > 0) {
      chunks.push(e.data);
    }
  };

  mediaRecorder.onerror = (e: Event) => {
    const error = (e as any).error;
    recordingCallback?.({
      isRecording: false,
      duration: 0,
      error: error?.message || 'Recording error'
    });
  };

  // Start recording (request data every second for progress)
  mediaRecorder.start(1000);
  startTime = Date.now();

  // Start duration timer
  timerInterval = setInterval(() => {
    const elapsed = Math.floor((Date.now() - startTime) / 1000);
    recordingCallback?.({
      isRecording: true,
      duration: elapsed,
      error: null
    });
  }, 500);

  // Auto-stop after max duration
  setTimeout(() => {
    if (mediaRecorder && mediaRecorder.state === 'recording') {
      stopRecording();
    }
  }, MAX_DURATION_MS);

  recordingCallback?.({
    isRecording: true,
    duration: 0,
    error: null
  });
}

/**
 * Stop recording and return the audio Blob.
 * @returns Promise<{ blob: Blob; mimeType: string; durationSec: number }>
 */
export function stopRecording(): Promise<{ blob: Blob; mimeType: string; durationSec: number }> {
  return new Promise((resolve, reject) => {
    if (!mediaRecorder || mediaRecorder.state !== 'recording') {
      reject(new Error('Not recording'));
      return;
    }

    const mimeType = mediaRecorder.mimeType;
    const durationSec = Math.floor((Date.now() - startTime) / 1000);

    // Clear timer
    if (timerInterval) {
      clearInterval(timerInterval);
      timerInterval = null;
    }

    mediaRecorder.onstop = () => {
      // Release microphone
      const stream = mediaRecorder?.stream;
      if (stream) {
        stream.getTracks().forEach(track => track.stop());
      }

      const blob = new Blob(chunks, { type: mimeType });
      chunks = [];
      mediaRecorder = null;

      recordingCallback?.({
        isRecording: false,
        duration: durationSec,
        error: null
      });

      resolve({ blob, mimeType, durationSec });
    };

    mediaRecorder.stop();
  });
}

/**
 * Format seconds as MM:SS string.
 */
export function formatDuration(seconds: number): string {
  const m = Math.floor(seconds / 60);
  const s = seconds % 60;
  return `${m}:${String(s).padStart(2, '0')}`;
}
```

**Create portfolio/src/scripts/whisper-client.ts:**

```typescript
/**
 * Client-side Whisper API communication.
 * Sends audio to the standalone transcription server endpoint.
 * Never exposes API key — all transcription goes through server.
 */

// Transcription server URL (configurable via env or default)
const TRANSCRIBE_URL = import.meta.env.PUBLIC_TRANSCRIBE_URL || 'http://localhost:3001/api/transcribe';

export interface TranscriptionResult {
  text: string;
  error: string | null;
}

/**
 * Send audio blob to Whisper API via server endpoint for transcription.
 * @param audioBlob - The recorded audio Blob
 * @param mimeType - MIME type of the audio (e.g., 'audio/webm;codecs=opus')
 * @returns TranscriptionResult with text or error
 */
export async function transcribeAudio(
  audioBlob: Blob,
  mimeType: string
): Promise<TranscriptionResult> {
  // Determine file extension from MIME type
  const extMap: Record<string, string> = {
    'audio/webm': 'webm',
    'audio/webm;codecs=opus': 'webm',
    'audio/mp4': 'm4a',
    'audio/ogg;codecs=opus': 'ogg',
    'audio/wav': 'wav'
  };
  const ext = extMap[mimeType] || 'webm';

  try {
    // Check if we're online
    if (!navigator.onLine) {
      return {
        text: '',
        error: 'offline'
      };
    }

    const formData = new FormData();
    const audioFile = new File([audioBlob], `voice-note.${ext}`, { type: mimeType });
    formData.append('audio', audioFile);

    const response = await fetch(TRANSCRIBE_URL, {
      method: 'POST',
      body: formData
    });

    if (!response.ok) {
      const errorData = await response.json().catch(() => ({}));
      return {
        text: '',
        error: errorData.message || `Transcription failed (${response.status})`
      };
    }

    const data = await response.json();
    return {
      text: data.text || '',
      error: null
    };

  } catch (error) {
    console.error('[whisper-client] Transcription error:', error);
    return {
      text: '',
      error: error instanceof Error ? error.message : 'Unknown error'
    };
  }
}
```
  </action>
  <verify>
1. portfolio/src/scripts/voice-recorder.ts exists with startRecording, stopRecording, detectMimeType exports
2. portfolio/src/scripts/whisper-client.ts exists with transcribeAudio export
3. detectMimeType() checks multiple MIME types including iOS Safari variants
4. Test on iOS Safari (or BrowserStack/simulator): confirm detectMimeType() returns a supported type (audio/webm;codecs=opus on Chrome/Android, or audio/mp4 on iOS Safari) — verify the fallback chain actually resolves on iOS
5. startRecording() requests microphone with echoCancellation and noiseSuppression
6. stopRecording() releases microphone tracks (stream.getTracks().forEach(stop))
7. Auto-stop after 5 minutes implemented
8. transcribeAudio() sends FormData to server endpoint (not direct to OpenAI)
9. Offline detection returns 'offline' error instead of attempting network request
10. `cd portfolio && npx tsc --noEmit` compiles without errors (or check via build)
  </verify>
  <done>
Voice recorder engine created with MediaRecorder API wrapper supporting iOS Safari MIME type detection (webm;codecs=opus, webm, mp4, ogg, wav fallback chain). Recording lifecycle managed with start/stop/auto-stop (5 min limit), duration timer, microphone release, and error handling. Whisper client sends audio via FormData to standalone server endpoint. Offline detection prevents failed network requests. API key never exposed to client.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create VoiceRecorder and VoiceNoteList components</name>
  <files>
    portfolio/src/components/VoiceRecorder.astro
    portfolio/src/components/VoiceNoteList.astro
    portfolio/src/pages/mobile/[slug].astro
  </files>
  <action>
Create the UI components for recording and viewing voice notes, and integrate them into the atom detail view.

**Create portfolio/src/components/VoiceRecorder.astro:**

```astro
---
/**
 * VoiceRecorder: Tap to record, tap to stop, auto-transcribe with Whisper.
 * Shows recording indicator, timer, and transcript review UI.
 */
export interface Props {
  atomSlug: string;
}

const { atomSlug } = Astro.props;
---

<div class="voice-recorder" data-atom-slug={atomSlug}>
  <!-- Record button -->
  <button id="record-btn" class="record-btn tap-target" aria-label="Record voice note">
    <span class="record-icon" id="record-icon"></span>
    <span class="record-label" id="record-label">Tap to Record</span>
  </button>

  <!-- Recording indicator -->
  <div id="recording-indicator" class="recording-indicator" style="display: none;">
    <span class="recording-pulse"></span>
    <span class="recording-timer" id="recording-timer">0:00</span>
    <span class="recording-hint">Tap to stop</span>
  </div>

  <!-- Transcription state -->
  <div id="transcribing-state" class="transcribing-state" style="display: none;">
    <span class="transcribing-spinner"></span>
    <span>Transcribing...</span>
  </div>

  <!-- Transcript review -->
  <div id="transcript-review" class="transcript-review" style="display: none;">
    <label class="transcript-label">Transcript (edit if needed):</label>
    <textarea id="transcript-text" class="transcript-textarea" rows="4" placeholder="Transcript will appear here..."></textarea>
    <div class="transcript-actions">
      <button id="save-note" class="btn btn-primary" style="flex: 1;">Save</button>
      <button id="discard-note" class="btn btn-secondary" style="flex: 1;">Discard</button>
    </div>
  </div>

  <!-- Error display -->
  <div id="recorder-error" class="recorder-error" style="display: none;">
    <span id="error-message"></span>
  </div>

  <!-- Feature unavailable -->
  <div id="recorder-unavailable" class="recorder-unavailable" style="display: none;">
    Voice recording is not available on this device.
  </div>
</div>

<script>
  import { isRecordingAvailable, startRecording, stopRecording, formatDuration } from '../scripts/voice-recorder';
  import { transcribeAudio } from '../scripts/whisper-client';
  import { saveVoiceNote } from '../scripts/db';

  async function initVoiceRecorder() {
    const container = document.querySelector('.voice-recorder') as HTMLElement;
    if (!container) return;

    const atomSlug = container.dataset.atomSlug || '';

    const recordBtn = document.getElementById('record-btn')!;
    const recordIcon = document.getElementById('record-icon')!;
    const recordLabel = document.getElementById('record-label')!;
    const recordingIndicator = document.getElementById('recording-indicator')!;
    const recordingTimer = document.getElementById('recording-timer')!;
    const transcribingState = document.getElementById('transcribing-state')!;
    const transcriptReview = document.getElementById('transcript-review')!;
    const transcriptText = document.getElementById('transcript-text') as HTMLTextAreaElement;
    const saveBtn = document.getElementById('save-note')!;
    const discardBtn = document.getElementById('discard-note')!;
    const recorderError = document.getElementById('recorder-error')!;
    const errorMessage = document.getElementById('error-message')!;
    const unavailable = document.getElementById('recorder-unavailable')!;

    // Check device support
    if (!isRecordingAvailable()) {
      recordBtn.style.display = 'none';
      unavailable.style.display = 'block';
      return;
    }

    let isRecording = false;
    let pendingAudioBlob: Blob | null = null;
    let pendingMimeType = '';

    // Record/stop toggle
    recordBtn.addEventListener('click', async () => {
      if (!isRecording) {
        try {
          recorderError.style.display = 'none';
          transcriptReview.style.display = 'none';

          await startRecording((state) => {
            if (state.isRecording) {
              recordingTimer.textContent = formatDuration(state.duration);
            }
            if (state.error) {
              showError(state.error);
            }
          });

          isRecording = true;
          recordBtn.classList.add('recording');
          recordIcon.classList.add('recording');
          recordLabel.textContent = 'Tap to Stop';
          recordingIndicator.style.display = 'flex';

        } catch (err) {
          showError(err instanceof Error ? err.message : 'Failed to start recording');
        }
      } else {
        try {
          const result = await stopRecording();

          isRecording = false;
          recordBtn.classList.remove('recording');
          recordIcon.classList.remove('recording');
          recordLabel.textContent = 'Tap to Record';
          recordingIndicator.style.display = 'none';

          pendingAudioBlob = result.blob;
          pendingMimeType = result.mimeType;

          // Auto-transcribe
          transcribingState.style.display = 'flex';
          const transcription = await transcribeAudio(result.blob, result.mimeType);
          transcribingState.style.display = 'none';

          if (transcription.error === 'offline') {
            transcriptText.value = '[Offline - transcription will be available when online]';
          } else if (transcription.error) {
            transcriptText.value = `[Transcription unavailable: ${transcription.error}]`;
          } else {
            transcriptText.value = transcription.text;
          }

          transcriptReview.style.display = 'block';

        } catch (err) {
          showError(err instanceof Error ? err.message : 'Failed to stop recording');
        }
      }
    });

    // Save voice note
    saveBtn.addEventListener('click', async () => {
      if (!pendingAudioBlob) return;

      await saveVoiceNote({
        atomSlug,
        audioBlob: pendingAudioBlob,
        mimeType: pendingMimeType,
        transcript: transcriptText.value,
        createdAt: new Date().toISOString(),
        synced: false
      });

      pendingAudioBlob = null;
      transcriptReview.style.display = 'none';

      // Reload voice notes list
      window.dispatchEvent(new CustomEvent('eoe:voice-notes-updated', {
        detail: { atomSlug }
      }));
    });

    // Discard voice note
    discardBtn.addEventListener('click', () => {
      pendingAudioBlob = null;
      transcriptReview.style.display = 'none';
    });

    function showError(message: string) {
      errorMessage.textContent = message;
      recorderError.style.display = 'block';
      isRecording = false;
      recordBtn.classList.remove('recording');
      recordIcon.classList.remove('recording');
      recordLabel.textContent = 'Tap to Record';
      recordingIndicator.style.display = 'none';
      transcribingState.style.display = 'none';
    }
  }

  initVoiceRecorder();
</script>

<style>
  .voice-recorder {
    padding: 16px 0;
  }

  .record-btn {
    width: 100%;
    min-height: 64px;
    display: flex;
    align-items: center;
    justify-content: center;
    gap: 12px;
    background: #1a1a1a;
    border: 2px solid #333;
    border-radius: 12px;
    color: #e0e0e0;
    font-size: 1rem;
    font-weight: 600;
    cursor: pointer;
    transition: all 0.2s;
  }

  .record-btn:active {
    background: #222;
  }

  .record-btn.recording {
    border-color: #ff4444;
    background: #1a0a0a;
  }

  .record-icon {
    width: 24px;
    height: 24px;
    border-radius: 50%;
    background: #ff4444;
    transition: all 0.2s;
  }

  .record-icon.recording {
    border-radius: 4px;
    width: 20px;
    height: 20px;
  }

  .recording-indicator {
    display: flex;
    align-items: center;
    justify-content: center;
    gap: 12px;
    padding: 12px;
    margin-top: 12px;
    background: #1a0a0a;
    border-radius: 8px;
    border: 1px solid #332222;
  }

  .recording-pulse {
    width: 10px;
    height: 10px;
    border-radius: 50%;
    background: #ff4444;
    animation: pulse 1s infinite;
  }

  @keyframes pulse {
    0%, 100% { opacity: 1; }
    50% { opacity: 0.3; }
  }

  .recording-timer {
    font-family: 'SF Mono', monospace;
    font-size: 1.2rem;
    font-weight: 600;
    color: #ff4444;
  }

  .recording-hint {
    font-size: 0.8rem;
    color: #888;
  }

  .transcribing-state {
    display: flex;
    align-items: center;
    justify-content: center;
    gap: 12px;
    padding: 16px;
    margin-top: 12px;
    color: #888;
  }

  .transcribing-spinner {
    width: 16px;
    height: 16px;
    border: 2px solid #333;
    border-top-color: #6bb5ff;
    border-radius: 50%;
    animation: spin 0.8s linear infinite;
  }

  @keyframes spin {
    to { transform: rotate(360deg); }
  }

  .transcript-review {
    margin-top: 16px;
  }

  .transcript-label {
    display: block;
    font-size: 0.85rem;
    color: #888;
    margin-bottom: 8px;
  }

  .transcript-textarea {
    width: 100%;
    padding: 12px;
    font-size: 16px; /* Prevents iOS zoom */
    background: #1a1a1a;
    border: 1px solid #333;
    border-radius: 8px;
    color: #e0e0e0;
    resize: vertical;
    font-family: -apple-system, BlinkMacSystemFont, sans-serif;
    line-height: 1.5;
  }

  .transcript-textarea:focus {
    border-color: #6bb5ff;
    outline: none;
  }

  .transcript-actions {
    display: flex;
    gap: 12px;
    margin-top: 12px;
  }

  .recorder-error {
    margin-top: 12px;
    padding: 12px;
    background: #1a0a0a;
    border: 1px solid #442222;
    border-radius: 8px;
    color: #ff6666;
    font-size: 0.85rem;
    text-align: center;
  }

  .recorder-unavailable {
    padding: 16px;
    text-align: center;
    color: #555;
    font-style: italic;
  }
</style>
```

**Create portfolio/src/components/VoiceNoteList.astro:**

```astro
---
/**
 * VoiceNoteList: Shows previously recorded voice notes for an atom.
 * Each note has playback, transcript display, and delete option.
 */
export interface Props {
  atomSlug: string;
}

const { atomSlug } = Astro.props;
---

<div class="voice-note-list" data-atom-slug={atomSlug}>
  <h4 class="voice-notes-heading">Voice Notes</h4>
  <div id="voice-notes-container">
    <p class="loading-text">Loading voice notes...</p>
  </div>
</div>

<script>
  import { getVoiceNotesForAtom, type VoiceNote } from '../scripts/db';

  async function loadVoiceNotes() {
    const container = document.querySelector('.voice-note-list') as HTMLElement;
    if (!container) return;

    const atomSlug = container.dataset.atomSlug || '';
    const notesContainer = document.getElementById('voice-notes-container')!;

    async function render() {
      const notes = await getVoiceNotesForAtom(atomSlug);

      if (notes.length === 0) {
        notesContainer.innerHTML = '<p class="empty-text">No voice notes yet. Tap record to start.</p>';
        return;
      }

      notesContainer.innerHTML = '';
      for (const note of notes.reverse()) { // Newest first
        const el = createNoteElement(note);
        notesContainer.appendChild(el);
      }
    }

    function createNoteElement(note: VoiceNote): HTMLElement {
      const div = document.createElement('div');
      div.className = 'voice-note-item';

      const date = new Date(note.createdAt);
      const dateStr = date.toLocaleDateString(undefined, {
        month: 'short', day: 'numeric', hour: '2-digit', minute: '2-digit'
      });

      // Audio playback
      const audioUrl = URL.createObjectURL(note.audioBlob);

      div.innerHTML = `
        <div class="note-header">
          <span class="note-date">${dateStr}</span>
        </div>
        <audio controls src="${audioUrl}" class="note-audio"></audio>
        <p class="note-transcript">${note.transcript || '<em>No transcript</em>'}</p>
      `;

      return div;
    }

    // Initial load
    await render();

    // Re-render when new note saved
    window.addEventListener('eoe:voice-notes-updated', async (e: CustomEvent) => {
      if (e.detail.atomSlug === atomSlug) {
        await render();
      }
    });
  }

  loadVoiceNotes();
</script>

<style>
  .voice-note-list {
    padding: 16px 0;
  }

  .voice-notes-heading {
    font-size: 0.95rem;
    color: #fff;
    margin-bottom: 12px;
    font-weight: 600;
  }

  .loading-text,
  .empty-text {
    color: #555;
    font-size: 0.85rem;
    font-style: italic;
  }

  .voice-note-item {
    background: #111;
    border-radius: 8px;
    padding: 12px;
    margin-bottom: 12px;
  }

  .note-header {
    display: flex;
    justify-content: space-between;
    align-items: center;
    margin-bottom: 8px;
  }

  .note-date {
    font-size: 0.75rem;
    color: #666;
  }

  .note-audio {
    width: 100%;
    height: 36px;
    margin-bottom: 8px;
  }

  .note-transcript {
    font-size: 0.85rem;
    color: #ccc;
    line-height: 1.5;
    white-space: pre-wrap;
  }
</style>
```

**Update portfolio/src/pages/mobile/[slug].astro:**

Add a "Voice" tab to the detail view. In the existing [slug].astro file:

1. Add imports in the frontmatter:
```javascript
import VoiceRecorder from '../../components/VoiceRecorder.astro';
import VoiceNoteList from '../../components/VoiceNoteList.astro';
```

2. Add "Voice" button to the tab navigation:
```html
<button class="tab" data-tab="voice">Voice</button>
```

3. Add the voice tab content panel:
```html
<!-- Voice tab -->
<div class="tab-content" id="tab-voice" style="display: none;">
  <VoiceRecorder atomSlug={atom.slug} />
  <VoiceNoteList atomSlug={atom.slug} />
</div>
```

Tab order should now be: Code | Config | Notes | Params | Voice
  </action>
  <verify>
1. portfolio/src/components/VoiceRecorder.astro exists with record button and transcript review UI
2. portfolio/src/components/VoiceNoteList.astro exists with voice note listing and playback
3. Detail view has 5 tabs: Code | Config | Notes | Params | Voice
4. `cd portfolio && npm run build` succeeds
5. Voice tab shows record button on page load
6. Record button has distinct recording state (red border, square icon)
7. Recording indicator shows pulsing dot and timer
8. Transcript review shows textarea with Save/Discard buttons
9. Voice notes list shows "No voice notes yet" when empty
  </verify>
  <done>
VoiceRecorder component created with tap-to-record/stop button, recording pulse indicator with timer, auto-transcription via Whisper server endpoint, transcript review/edit textarea, and save/discard actions. VoiceNoteList shows previous recordings with audio playback and transcripts. Both integrated into atom detail view as Voice tab. Recording saves to IndexedDB, offline recording saves audio with placeholder transcript.
  </done>
</task>

<task type="auto">
  <name>Task 4: Verify voice recording and transcription end-to-end</name>
  <files>
    (no new files -- verification task)
  </files>
  <action>
Verify the voice recording and transcription pipeline works end-to-end.

**Prerequisites:**
- Set OPENAI_API_KEY environment variable with a valid OpenAI key
- Start transcription server: `cd portfolio && OPENAI_API_KEY=$OPENAI_API_KEY npm run transcribe`
- Start portfolio dev server: `cd portfolio && npm run dev`

**Verification checks:**

1. **Device support detection:**
   - Open /mobile/<any-atom> and switch to Voice tab
   - On a device with microphone: record button should appear
   - In an environment without MediaRecorder: "Voice recording is not available" message should appear

2. **Recording:**
   - Tap "Tap to Record" button
   - Browser should request microphone permission (grant it)
   - Button should change to recording state (red border, square icon)
   - Recording indicator should show pulsing dot and timer counting up
   - Speak a few words clearly

3. **Stop and transcribe:**
   - Tap "Tap to Stop"
   - Recording indicator hides
   - "Transcribing..." spinner appears
   - After a few seconds, transcript appears in textarea
   - Verify transcript roughly matches what was spoken (>80% accuracy target)

4. **Edit and save:**
   - Edit the transcript text if needed (fix any errors)
   - Tap "Save"
   - Transcript review hides
   - Voice Notes list below should now show the new note
   - New note shows date, audio player, and transcript

5. **Audio playback:**
   - Tap the audio player on the saved voice note
   - Verify audio plays back correctly

6. **Offline behavior:**
   - Turn off network (DevTools > Network > Offline)
   - Record a new voice note
   - Stop recording
   - Verify transcript shows "[Offline - transcription will be available when online]"
   - Save the note
   - Verify audio is saved in IndexedDB (playback works even offline)

7. **Persistence:**
   - Reload the page
   - Navigate to Voice tab
   - Verify saved voice notes appear in the list
   - Verify audio playback still works

8. **Discard:**
   - Record a new note
   - In transcript review, tap "Discard"
   - Verify the note is not saved to the list

9. **Safety limit:**
   - Start recording and wait (or verify in code) that it auto-stops after 5 minutes

10. **iOS Safari MIME type (if iOS device available):**
    - Test on iPhone Safari
    - Verify recording starts without errors
    - Check that MIME type detection found a supported format

If transcription server is not running, voice recording should still work — audio saves, but transcript shows error message.
  </action>
  <verify>
1. Record button works: tap to start, tap to stop
2. Recording indicator shows pulsing dot and timer
3. Whisper transcription returns readable text
4. Transcript editable before saving
5. Saved voice notes appear in list with audio playback
6. Audio plays back correctly from IndexedDB Blob
7. Offline recording saves audio with placeholder transcript
8. Voice notes persist across page reloads
9. Discard button prevents saving
10. Auto-stop at 5 minutes works
  </verify>
  <done>
Voice recording and transcription verified end-to-end: tap to record/stop works, recording indicator shows timer, Whisper API transcribes with >80% accuracy, transcript editable before save, saved notes listed with audio playback, offline recording saves audio with placeholder, notes persist across reloads, discard works, 5-minute auto-stop works. IDEA-01 (voice capture) and IDEA-02 (transcription) functional.
  </done>
</task>

</tasks>

<verification>
1. Tap-to-record/stop cycle works without errors
2. Recording indicator shows pulsing red dot and timer
3. Auto-stop triggers at 5 minutes
4. Whisper API transcription returns readable text (>80% accuracy)
5. Transcript review/edit UI shows before saving
6. Voice notes saved to IndexedDB with audio Blob + transcript
7. Voice notes list shows saved recordings with audio playback
8. Offline recording saves audio, shows offline transcript placeholder
9. MediaRecorder MIME type detection works on iOS Safari (webm;codecs=opus fallback)
10. API key never exposed to client (all transcription via server endpoint)
</verification>

<success_criteria>
- User can capture voice notes on mobile by tapping record/stop (IDEA-01)
- Voice notes auto-transcribe via Whisper API with >80% accuracy (IDEA-02)
- API key stays server-side, never exposed to mobile client
- User can review and edit transcript before saving
- Previously recorded voice notes are listed with audio playback
- Recording works offline (audio saved, transcription deferred)
- MediaRecorder supports iOS Safari via MIME type detection fallback chain
- Recording auto-stops at 5 minutes as safety limit
- All voice note data stored in IndexedDB voiceNotes store
</success_criteria>

<output>
After completion, create `.planning/phases/04-mobile-gallery-ideation-tools/04-04-SUMMARY.md`
</output>
