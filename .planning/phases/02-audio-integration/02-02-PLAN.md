---
phase: 02-audio-integration
plan: 02
type: execute
wave: 1
depends_on: ["02-01"]
files_modified:
  - lib/audio/analyser.js
  - lib/audio/bands.js
  - lib/audio/beat-detect.js
  - lib/audio/envelope.js
  - lib/audio/smoothing.js
  - lib/audio/audio-data.js
  - lib/audio/index.js
  - cli/templates/audio-visual/index.html
  - cli/templates/audio-visual/sketch.js
  - cli/templates/audio-visual/audio.js
  - cli/templates/audio-visual/config.json
  - cli/templates/audio-visual/NOTES.md
  - cli/commands/create.js
autonomous: true

must_haves:
  truths:
    - "Tone.Analyser produces normalized 0-1 frequency data updated every frame"
    - "Frequency bands (bass, lowMid, mid, highMid, treble) extracted from FFT spectrum"
    - "Beat detection fires on transient energy spikes using spectral flux algorithm"
    - "Envelope follower tracks overall loudness with attack/release smoothing"
    - "Audio data object { bass, lowMid, mid, highMid, treble, energy, beat, envelope } is available per frame"
    - "Audio data values are smoothed with exponential moving average by default"
    - "Easing functions (linear, exponential, logarithmic, sine) transform 0-1 audio values to visual parameters"
    - "User can run `eoe create audio-visual <name>` and get a combined audio+visual atom"
    - "Audio-visual atom demonstrates p5.js sketch reacting to audio analysis in real time"
  artifacts:
    - path: "lib/audio/analyser.js"
      provides: "Tone.Analyser wrapper with normalized 0-1 output"
      contains: "Tone.Analyser"
    - path: "lib/audio/bands.js"
      provides: "Frequency band extraction from FFT spectrum"
      contains: "extractBands"
    - path: "lib/audio/beat-detect.js"
      provides: "Spectral flux beat detection"
      contains: "detectBeat"
    - path: "lib/audio/envelope.js"
      provides: "Envelope follower for loudness tracking"
      contains: "envelopeFollower"
    - path: "lib/audio/smoothing.js"
      provides: "Exponential smoothing and easing curve utilities"
      contains: "smooth"
    - path: "lib/audio/audio-data.js"
      provides: "Aggregated audio data object updated per frame"
      contains: "AudioDataProvider"
    - path: "cli/templates/audio-visual/sketch.js"
      provides: "Audio-reactive p5.js sketch template"
      contains: "audioData"
    - path: "cli/templates/audio-visual/audio.js"
      provides: "Audio setup with analysis for audio-visual atom"
      contains: "getAudioData"
  key_links:
    - from: "lib/audio/audio-data.js"
      to: "lib/audio/analyser.js"
      via: "import createAnalyser"
      pattern: "import.*analyser"
    - from: "lib/audio/audio-data.js"
      to: "lib/audio/bands.js"
      via: "import extractBands"
      pattern: "import.*bands"
    - from: "lib/audio/audio-data.js"
      to: "lib/audio/beat-detect.js"
      via: "import BeatDetector"
      pattern: "import.*beat"
    - from: "lib/audio/audio-data.js"
      to: "lib/audio/envelope.js"
      via: "import EnvelopeFollower"
      pattern: "import.*envelope"
    - from: "cli/templates/audio-visual/sketch.js"
      to: "cli/templates/audio-visual/audio.js"
      via: "import getAudioData"
      pattern: "import.*audio"
    - from: "cli/templates/audio-visual/sketch.js"
      to: "lib/audio/smoothing.js"
      via: "import easing functions"
      pattern: "import.*smoothing"
---

<objective>
Build the audio analysis pipeline and audio-visual binding layer. After this plan, audio atoms produce per-frame analysis data (frequency bands, beat detection, envelope following) normalized to 0-1 range, and a new audio-visual atom template demonstrates p5.js sketches reacting to audio in real time.

Purpose: This is the bridge between audio and visual worlds. AUD-03 (audio-reactive visual parameters) is the key differentiator for Phase 2 -- without it, audio and visual atoms remain isolated. The analysis pipeline converts raw FFT data into meaningful, smooth values that drive visual properties.
Output: Audio analysis library modules (analyser, bands, beat detection, envelope, smoothing), an AudioDataProvider class that aggregates all metrics, and a working audio-visual atom template with a reactive demo sketch.
</objective>

<execution_context>
@/home/pavel/.claude/get-shit-done/workflows/execute-plan.md
@/home/pavel/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-audio-integration/02-CONTEXT.md
@.planning/phases/02-audio-integration/02-RESEARCH.md
@.planning/phases/02-audio-integration/02-01-PLAN.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create audio analysis modules (analyser, bands, beat detection, envelope)</name>
  <files>
    lib/audio/analyser.js
    lib/audio/bands.js
    lib/audio/beat-detect.js
    lib/audio/envelope.js
  </files>
  <action>
Create the core audio analysis modules that extract meaningful metrics from Tone.js audio.

**lib/audio/analyser.js** -- Tone.Analyser wrapper with normalized output:

```javascript
import * as Tone from 'tone';

/**
 * Create a Tone.Analyser configured for FFT analysis with normalized 0-1 output.
 *
 * @param {object} options - { fftSize, smoothingTimeConstant }
 * @returns {object} { analyser, connect(source), getValue(), dispose() }
 */
export function createAnalyser(options = {}) {
  const {
    fftSize = 1024,
    smoothingTimeConstant = 0.8
  } = options;

  const analyser = new Tone.Analyser('fft', fftSize);
  analyser.smoothing = smoothingTimeConstant;

  return {
    analyser,

    /**
     * Connect an audio source to this analyser.
     * @param {object} source - Tone.js audio node
     */
    connect(source) {
      source.connect(analyser);
      return this;
    },

    /**
     * Get current FFT data as Float32Array (dB values).
     * @returns {Float32Array}
     */
    getRawValue() {
      return analyser.getValue();
    },

    /**
     * Get current FFT data normalized to 0-1 range.
     * Tone.Analyser returns dB values (typically -100 to 0).
     * We map to 0-1 using minDecibels/maxDecibels.
     *
     * @param {number} minDb - Minimum dB value (maps to 0). Default: -100
     * @param {number} maxDb - Maximum dB value (maps to 1). Default: -30
     * @returns {Float32Array} Values in 0-1 range
     */
    getNormalizedValue(minDb = -100, maxDb = -30) {
      const raw = analyser.getValue();
      const range = maxDb - minDb;
      const normalized = new Float32Array(raw.length);

      for (let i = 0; i < raw.length; i++) {
        normalized[i] = Math.max(0, Math.min(1, (raw[i] - minDb) / range));
      }

      return normalized;
    },

    /**
     * Get the FFT size (number of frequency bins = fftSize / 2).
     */
    get binCount() {
      return fftSize / 2;
    },

    dispose() {
      analyser.dispose();
    }
  };
}
```

**lib/audio/bands.js** -- Frequency band extraction:

```javascript
/**
 * Extract frequency bands from normalized FFT spectrum.
 * Frequency ranges based on standard audio bands:
 *   Bass:    20-250 Hz
 *   Low-mid: 250-500 Hz
 *   Mid:     500-2000 Hz
 *   High-mid: 2000-4000 Hz
 *   Treble:  4000-8000 Hz
 *
 * @param {Float32Array} spectrum - Normalized FFT data (0-1 range)
 * @param {number} sampleRate - Audio context sample rate (default: 48000)
 * @param {number} fftSize - FFT size used (default: 1024)
 * @returns {object} { bass, lowMid, mid, highMid, treble, sub }
 */
export function extractBands(spectrum, sampleRate = 48000, fftSize = 1024) {
  const binWidth = sampleRate / fftSize;
  const binCount = spectrum.length;

  // Convert frequency to bin index
  const freqToBin = (freq) => Math.min(Math.round(freq / binWidth), binCount - 1);

  // Band frequency ranges
  const bands = {
    sub:     [20, 60],
    bass:    [60, 250],
    lowMid:  [250, 500],
    mid:     [500, 2000],
    highMid: [2000, 4000],
    treble:  [4000, 8000]
  };

  const result = {};

  for (const [name, [lowFreq, highFreq]] of Object.entries(bands)) {
    const lowBin = freqToBin(lowFreq);
    const highBin = freqToBin(highFreq);

    if (lowBin >= highBin) {
      result[name] = 0;
      continue;
    }

    // Average the energy across bins in this band
    let sum = 0;
    let count = 0;
    for (let i = lowBin; i <= highBin && i < binCount; i++) {
      sum += spectrum[i];
      count++;
    }

    result[name] = count > 0 ? sum / count : 0;
  }

  return result;
}

/**
 * Get simplified bands (bass, mids, treble) for common use.
 * Combines sub-bands into three main categories.
 *
 * @param {object} bands - Full band extraction from extractBands()
 * @returns {object} { bass, mids, treble }
 */
export function getSimpleBands(bands) {
  return {
    bass: Math.max(bands.sub || 0, bands.bass || 0),
    mids: ((bands.lowMid || 0) + (bands.mid || 0) + (bands.highMid || 0)) / 3,
    treble: bands.treble || 0
  };
}
```

**lib/audio/beat-detect.js** -- Spectral flux beat detection:

```javascript
/**
 * Beat detector using spectral flux algorithm.
 * Tracks energy changes between frames and fires on positive transients.
 *
 * Usage:
 *   const detector = new BeatDetector();
 *   // In animation loop:
 *   const isBeat = detector.update(spectrum);
 */
export class BeatDetector {
  /**
   * @param {object} options
   * @param {number} options.threshold - Flux threshold for beat detection (default: 0.15)
   * @param {number} options.decayRate - How fast the threshold adapts (default: 0.98)
   * @param {number} options.minInterval - Minimum ms between beats (default: 200)
   */
  constructor(options = {}) {
    this.threshold = options.threshold ?? 0.15;
    this.decayRate = options.decayRate ?? 0.98;
    this.minInterval = options.minInterval ?? 200;

    this.prevSpectrum = null;
    this.adaptiveThreshold = this.threshold;
    this.lastBeatTime = 0;
    this.beatValue = 0;        // 0-1: 1 at beat, decays to 0
    this.beatDecay = 0.9;      // How fast beat value decays per frame
  }

  /**
   * Update beat detector with new spectrum data.
   *
   * @param {Float32Array} spectrum - Normalized 0-1 FFT data
   * @returns {boolean} True if a beat was detected this frame
   */
  update(spectrum) {
    // Decay beat value each frame
    this.beatValue *= this.beatDecay;

    if (!this.prevSpectrum) {
      this.prevSpectrum = new Float32Array(spectrum);
      return false;
    }

    // Calculate spectral flux (positive differences only)
    let flux = 0;
    for (let i = 0; i < spectrum.length; i++) {
      const diff = spectrum[i] - this.prevSpectrum[i];
      if (diff > 0) flux += diff;
    }

    // Normalize flux by number of bins
    flux /= spectrum.length;

    // Adaptive threshold: slowly decays toward base threshold
    this.adaptiveThreshold = Math.max(
      this.threshold,
      this.adaptiveThreshold * this.decayRate
    );

    // Check for beat
    const now = performance.now();
    const isBeat = flux > this.adaptiveThreshold &&
                   (now - this.lastBeatTime) > this.minInterval;

    if (isBeat) {
      this.lastBeatTime = now;
      this.adaptiveThreshold = flux * 1.1; // Raise threshold after beat
      this.beatValue = 1.0; // Reset to 1 on beat
    }

    // Store current spectrum for next frame
    this.prevSpectrum.set(spectrum);

    return isBeat;
  }

  /**
   * Get current beat value (0-1).
   * 1.0 at moment of beat, decays exponentially to 0.
   * Use this for smooth visual reactions to beats.
   */
  getValue() {
    return this.beatValue;
  }

  /**
   * Reset detector state.
   */
  reset() {
    this.prevSpectrum = null;
    this.adaptiveThreshold = this.threshold;
    this.lastBeatTime = 0;
    this.beatValue = 0;
  }
}
```

**lib/audio/envelope.js** -- Envelope follower for loudness tracking:

```javascript
/**
 * Envelope follower that tracks overall audio energy level.
 * Uses different attack/release rates for smooth tracking.
 *
 * Usage:
 *   const follower = new EnvelopeFollower();
 *   // In animation loop:
 *   const level = follower.update(spectrum);
 */
export class EnvelopeFollower {
  /**
   * @param {object} options
   * @param {number} options.attack - Attack rate (0-1, how fast to rise). Default: 0.1
   * @param {number} options.release - Release rate (0-1, how fast to fall). Default: 0.05
   */
  constructor(options = {}) {
    this.attack = options.attack ?? 0.1;
    this.release = options.release ?? 0.05;
    this.currentLevel = 0;
    this.peakLevel = 0;
    this.peakDecay = 0.999; // Very slow peak decay for auto-gain
  }

  /**
   * Update envelope with current spectrum.
   *
   * @param {Float32Array} spectrum - Normalized 0-1 FFT data
   * @returns {number} Current envelope level (0-1)
   */
  update(spectrum) {
    // Calculate RMS energy
    let sumSquares = 0;
    for (let i = 0; i < spectrum.length; i++) {
      sumSquares += spectrum[i] * spectrum[i];
    }
    const rms = Math.sqrt(sumSquares / spectrum.length);

    // Track peak for auto-normalization
    if (rms > this.peakLevel) {
      this.peakLevel = rms;
    } else {
      this.peakLevel *= this.peakDecay;
    }

    // Normalize by peak (auto-gain)
    const targetLevel = this.peakLevel > 0.001 ? rms / this.peakLevel : 0;

    // Apply attack/release smoothing
    if (targetLevel > this.currentLevel) {
      this.currentLevel += (targetLevel - this.currentLevel) * this.attack;
    } else {
      this.currentLevel += (targetLevel - this.currentLevel) * this.release;
    }

    return Math.max(0, Math.min(1, this.currentLevel));
  }

  /**
   * Get current level without updating.
   * @returns {number} Current level (0-1)
   */
  getValue() {
    return this.currentLevel;
  }

  /**
   * Reset follower state.
   */
  reset() {
    this.currentLevel = 0;
    this.peakLevel = 0;
  }
}
```
  </action>
  <verify>
1. All four files exist: lib/audio/analyser.js, bands.js, beat-detect.js, envelope.js
2. `node -e "import('./lib/audio/analyser.js').then(m => console.log(Object.keys(m)))"` prints exports
3. `node -e "import('./lib/audio/bands.js').then(m => console.log(Object.keys(m)))"` prints exports
4. `node -e "import('./lib/audio/beat-detect.js').then(m => console.log(m.BeatDetector.name))"` prints 'BeatDetector'
5. `node -e "import('./lib/audio/envelope.js').then(m => console.log(m.EnvelopeFollower.name))"` prints 'EnvelopeFollower'
  </verify>
  <done>
Audio analysis modules created: analyser (Tone.Analyser wrapper with normalized 0-1 output), bands (frequency band extraction for bass/lowMid/mid/highMid/treble plus simple 3-band helper), beat-detect (spectral flux algorithm with adaptive threshold and beat-value decay), envelope (RMS energy follower with attack/release smoothing and auto-gain normalization).
  </done>
</task>

<task type="auto">
  <name>Task 2: Create smoothing utilities and AudioDataProvider</name>
  <files>
    lib/audio/smoothing.js
    lib/audio/audio-data.js
    lib/audio/index.js
  </files>
  <action>
Create smoothing/easing utilities and the AudioDataProvider that aggregates all analysis into a single per-frame data object.

**lib/audio/smoothing.js** -- Exponential smoothing and easing curves:

```javascript
/**
 * Smoothing and easing utilities for audio-to-visual parameter mapping.
 * All functions work with 0-1 normalized values.
 */

/**
 * Exponential moving average smoother.
 * Create one per parameter for persistent state.
 *
 * @param {number} alpha - Smoothing factor (0-1). Lower = smoother. Default: 0.15
 * @returns {function} smoother(newValue) -> smoothedValue
 */
export function createSmoother(alpha = 0.15) {
  let current = null;

  return function smooth(value) {
    if (current === null) {
      current = value;
      return value;
    }
    current += (value - current) * alpha;
    return current;
  };
}

/**
 * Batch smoother for an object of values.
 * Creates individual smoothers for each key.
 *
 * @param {string[]} keys - Keys to smooth
 * @param {number} alpha - Smoothing factor
 * @returns {function} smoothAll(dataObject) -> smoothedObject
 */
export function createBatchSmoother(keys, alpha = 0.15) {
  const smoothers = {};
  for (const key of keys) {
    smoothers[key] = createSmoother(alpha);
  }

  return function smoothAll(data) {
    const result = { ...data };
    for (const key of keys) {
      if (key in data && smoothers[key]) {
        result[key] = smoothers[key](data[key]);
      }
    }
    return result;
  };
}

// --- Easing Functions ---
// All take a 0-1 input and return 0-1 output with nonlinear mapping.

/** Linear (identity) */
export function easeLinear(t) {
  return t;
}

/** Exponential ease-in */
export function easeExponentialIn(t) {
  return t === 0 ? 0 : Math.pow(2, 10 * (t - 1));
}

/** Exponential ease-out */
export function easeExponentialOut(t) {
  return t === 1 ? 1 : 1 - Math.pow(2, -10 * t);
}

/** Logarithmic (soft response to quiet, compressed response to loud) */
export function easeLogarithmic(t) {
  return t <= 0 ? 0 : Math.log(1 + t * 9) / Math.log(10);
}

/** Sine ease-in-out (smooth S-curve) */
export function easeSineInOut(t) {
  return -(Math.cos(Math.PI * t) - 1) / 2;
}

/** Cubic ease-out (fast start, slow finish) */
export function easeCubicOut(t) {
  return 1 - Math.pow(1 - t, 3);
}

/** Quadratic ease-in (slow start, fast finish) */
export function easeQuadraticIn(t) {
  return t * t;
}

/**
 * Apply easing and range mapping to a 0-1 value.
 * Maps input through easing function, then scales to [min, max] range.
 *
 * @param {number} value - Input value (0-1)
 * @param {object} mapping - { min, max, curve }
 * @returns {number} Mapped value in [min, max] range
 */
export function applyMapping(value, mapping = {}) {
  const { min = 0, max = 1, curve = 'linear' } = mapping;

  // Clamp input
  const t = Math.max(0, Math.min(1, value));

  // Apply easing curve
  const easingFns = {
    linear: easeLinear,
    exponentialIn: easeExponentialIn,
    exponentialOut: easeExponentialOut,
    logarithmic: easeLogarithmic,
    sineInOut: easeSineInOut,
    cubicOut: easeCubicOut,
    quadraticIn: easeQuadraticIn
  };

  const easeFn = easingFns[curve] || easeLinear;
  const eased = easeFn(t);

  // Map to range
  return min + eased * (max - min);
}
```

**lib/audio/audio-data.js** -- Aggregated audio data provider:

```javascript
import { createAnalyser } from './analyser.js';
import { extractBands, getSimpleBands } from './bands.js';
import { BeatDetector } from './beat-detect.js';
import { EnvelopeFollower } from './envelope.js';
import { createBatchSmoother } from './smoothing.js';

/**
 * AudioDataProvider aggregates all audio analysis into a single per-frame data object.
 * Connect it to an audio source, then call update() every frame to get fresh data.
 *
 * Usage:
 *   const provider = new AudioDataProvider(synthNode);
 *   // In animation loop:
 *   const data = provider.update();
 *   // data = { bass, lowMid, mid, highMid, treble, energy, beat, envelope, ... }
 */
export class AudioDataProvider {
  /**
   * @param {object} source - Tone.js audio node to analyze
   * @param {object} options - Configuration options
   * @param {number} options.fftSize - FFT size (default: 1024)
   * @param {number} options.smoothing - Analyser smoothing (default: 0.8)
   * @param {number} options.outputSmoothing - Output value smoothing alpha (default: 0.15)
   * @param {number} options.beatThreshold - Beat detection threshold (default: 0.15)
   * @param {number} options.sampleRate - Audio sample rate (default: 48000)
   */
  constructor(source, options = {}) {
    const {
      fftSize = 1024,
      smoothing = 0.8,
      outputSmoothing = 0.15,
      beatThreshold = 0.15,
      sampleRate = 48000
    } = options;

    this.fftSize = fftSize;
    this.sampleRate = sampleRate;

    // Create analyser and connect to source
    this.analyser = createAnalyser({ fftSize, smoothingTimeConstant: smoothing });
    this.analyser.connect(source);

    // Create detectors
    this.beatDetector = new BeatDetector({ threshold: beatThreshold });
    this.envelopeFollower = new EnvelopeFollower();

    // Create batch smoother for output values
    this.smoother = createBatchSmoother(
      ['bass', 'lowMid', 'mid', 'highMid', 'treble', 'sub', 'energy', 'envelope'],
      outputSmoothing
    );

    // Current data (last update result)
    this.data = {
      bass: 0, lowMid: 0, mid: 0, highMid: 0, treble: 0, sub: 0,
      energy: 0, beat: 0, envelope: 0,
      // Simple 3-band aliases
      mids: 0,
      // Raw spectrum (unsmoothed, for advanced use)
      spectrum: new Float32Array(fftSize / 2)
    };
  }

  /**
   * Update all analysis and return fresh audio data.
   * Call this once per animation frame.
   *
   * @returns {object} Audio data with all metrics (0-1 normalized)
   */
  update() {
    // Get normalized spectrum
    const spectrum = this.analyser.getNormalizedValue();

    // Extract frequency bands
    const bands = extractBands(spectrum, this.sampleRate, this.fftSize);
    const simpleBands = getSimpleBands(bands);

    // Update beat detector
    this.beatDetector.update(spectrum);

    // Update envelope follower
    this.envelopeFollower.update(spectrum);

    // Calculate overall energy (average of all bands)
    const energy = (bands.sub + bands.bass + bands.lowMid + bands.mid + bands.highMid + bands.treble) / 6;

    // Assemble raw data
    const rawData = {
      ...bands,
      energy,
      envelope: this.envelopeFollower.getValue(),
      beat: this.beatDetector.getValue()
    };

    // Apply smoothing
    const smoothed = this.smoother(rawData);

    // Update stored data
    this.data = {
      ...smoothed,
      mids: simpleBands.mids,
      beat: this.beatDetector.getValue(), // Beat value not smoothed (already has its own decay)
      spectrum
    };

    return this.data;
  }

  /**
   * Get last computed data without updating.
   * @returns {object}
   */
  getData() {
    return this.data;
  }

  /**
   * Reset all detector states.
   */
  reset() {
    this.beatDetector.reset();
    this.envelopeFollower.reset();
  }

  /**
   * Dispose all resources.
   */
  dispose() {
    this.analyser.dispose();
    this.beatDetector.reset();
    this.envelopeFollower.reset();
  }
}
```

**Update lib/audio/index.js** -- Add new exports to barrel file:

Add the following exports to the existing barrel file:
```javascript
export { createAnalyser } from './analyser.js';
export { extractBands, getSimpleBands } from './bands.js';
export { BeatDetector } from './beat-detect.js';
export { EnvelopeFollower } from './envelope.js';
export { createSmoother, createBatchSmoother, applyMapping, easeLinear, easeExponentialIn, easeExponentialOut, easeLogarithmic, easeSineInOut, easeCubicOut, easeQuadraticIn } from './smoothing.js';
export { AudioDataProvider } from './audio-data.js';
```

These are appended to the existing exports from Plan 02-01 (createSynth, createEffectsChain, transport utils, disposeAll).
  </action>
  <verify>
1. All files exist: lib/audio/smoothing.js, audio-data.js
2. lib/audio/index.js exports all modules (old + new)
3. `node -e "import('./lib/audio/index.js').then(m => console.log(Object.keys(m)))"` shows all exports including AudioDataProvider, BeatDetector, etc.
4. `node -e "import('./lib/audio/smoothing.js').then(m => { const s = m.createSmoother(0.5); console.log(s(1), s(1)); })"` prints two smoothed values
5. `node -e "import('./lib/audio/smoothing.js').then(m => console.log(m.applyMapping(0.5, { min: 0, max: 100, curve: 'logarithmic' })))"` prints a mapped value
  </verify>
  <done>
Smoothing utilities created with exponential moving average (per-value and batch), 7 easing functions (linear, exponentialIn/Out, logarithmic, sineInOut, cubicOut, quadraticIn), and applyMapping for curve + range transformation. AudioDataProvider class aggregates analyser, bands, beat detection, and envelope follower into a single update() call returning { bass, lowMid, mid, highMid, treble, energy, beat, envelope, mids, spectrum }.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create audio-visual atom template and extend CLI</name>
  <files>
    cli/templates/audio-visual/index.html
    cli/templates/audio-visual/sketch.js
    cli/templates/audio-visual/audio.js
    cli/templates/audio-visual/config.json
    cli/templates/audio-visual/NOTES.md
    cli/commands/create.js
  </files>
  <action>
Create the audio-visual atom template -- a combined atom that has both Tone.js audio and p5.js visuals, with the visual sketch reacting to audio analysis data. Also extend CLI to support the `audio-visual` type.

**cli/templates/audio-visual/index.html** -- Combined audio-visual atom entry point:

```html
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>{{ATOM_NAME}}</title>
  <style>
    body {
      margin: 0;
      padding: 0;
      display: flex;
      flex-direction: column;
      align-items: center;
      min-height: 100vh;
      background: #0a0a0a;
      color: #e0e0e0;
      font-family: 'SF Mono', 'Fira Code', monospace;
    }
    .transport {
      display: flex;
      gap: 1rem;
      margin: 1rem 0;
      z-index: 10;
    }
    .transport button {
      padding: 0.5rem 1.5rem;
      border: 1px solid #333;
      border-radius: 6px;
      background: #151515;
      color: #e0e0e0;
      font-family: inherit;
      font-size: 0.9rem;
      cursor: pointer;
      transition: border-color 0.2s;
    }
    .transport button:hover { border-color: #555; }
    .transport button.active { border-color: #6bb5ff; color: #6bb5ff; }
  </style>
</head>
<body>
  <div class="transport">
    <button id="playBtn">Play</button>
    <button id="stopBtn">Stop</button>
  </div>
  <script type="module" src="./sketch.js"></script>
</body>
</html>
```

**cli/templates/audio-visual/audio.js** -- Audio setup with analysis:

```javascript
import * as Tone from 'tone';
import {
  createSynth,
  createEffectsChain,
  ensureAudioContext,
  startTransport,
  stopTransport,
  createSequence,
  disposeAll,
  AudioDataProvider
} from '../../lib/audio/index.js';

let synth = null;
let effectsChain = null;
let sequence = null;
let audioDataProvider = null;
let isPlaying = false;

/**
 * Initialize audio with given config.
 * @param {object} config - Audio configuration
 */
export function initAudio(config) {
  // Create effects chain
  effectsChain = createEffectsChain(config.effects || {});
  effectsChain.output.toDestination();

  // Create synth and connect to effects chain
  synth = createSynth(config.synth || {});
  synth.connect(effectsChain.chain);

  // Create audio data provider (connects analyser to synth output)
  audioDataProvider = new AudioDataProvider(effectsChain.output, {
    fftSize: config.analysis?.fftSize || 1024,
    smoothing: config.analysis?.smoothing || 0.8,
    outputSmoothing: config.analysis?.outputSmoothing || 0.15,
    beatThreshold: config.analysis?.beatThreshold || 0.15
  });

  // Create sequence
  sequence = createSequence(synth, config.sequence || {});
}

/**
 * Start audio playback.
 * @param {number} bpm - Beats per minute
 */
export async function startAudio(bpm = 120) {
  await ensureAudioContext();
  if (!isPlaying) {
    sequence.start(0);
    startTransport(bpm);
    isPlaying = true;
  }
}

/**
 * Stop audio playback.
 */
export function stopAudio() {
  if (isPlaying) {
    sequence.stop();
    stopTransport();
    isPlaying = false;
    if (audioDataProvider) audioDataProvider.reset();
  }
}

/**
 * Get current audio analysis data.
 * Call this every frame in the sketch's draw() function.
 *
 * @returns {object} { bass, lowMid, mid, highMid, treble, energy, beat, envelope, mids, spectrum }
 */
export function getAudioData() {
  if (!audioDataProvider) {
    return { bass: 0, lowMid: 0, mid: 0, highMid: 0, treble: 0, energy: 0, beat: 0, envelope: 0, mids: 0, spectrum: [] };
  }
  return audioDataProvider.update();
}

/**
 * Check if audio is currently playing.
 */
export function getIsPlaying() {
  return isPlaying;
}

/**
 * Clean up all audio resources.
 */
export async function cleanupAudio() {
  isPlaying = false;

  if (audioDataProvider) {
    audioDataProvider.dispose();
    audioDataProvider = null;
  }

  await disposeAll({
    sequences: [sequence],
    effects: effectsChain,
    synths: [synth]
  });

  synth = null;
  effectsChain = null;
  sequence = null;
}
```

**cli/templates/audio-visual/sketch.js** -- Audio-reactive p5.js sketch:

The demo sketch should demonstrate audio reactivity clearly: bass controls size, mids control color, treble controls rotation/detail, beats trigger flashes. This makes the audio-visual connection immediately obvious and inspiring.

```javascript
import p5 from 'p5';
import GUI from 'lil-gui';
import { initAudio, startAudio, stopAudio, getAudioData, cleanupAudio } from './audio.js';
import { applyMapping } from '../../lib/audio/smoothing.js';

let p5Instance;

const sketch = (p) => {
  // --- Config ---
  let config = {
    // Visual parameters
    bgHue: 240,
    baseSize: 100,
    particleCount: 60,
    rotationSpeed: 0.5,

    // Audio-visual mapping
    bassSizeScale: 200,
    midsHueShift: 120,
    trebleDetail: 8,
    beatFlash: 0.8,

    // Audio config
    synth: {
      type: 'mono',
      oscillator: { type: 'triangle' },
      envelope: { attack: 0.01, decay: 0.3, sustain: 0.4, release: 0.8 }
    },
    sequence: {
      notes: ['C4', 'E4', 'G4', 'B4', 'C5', null, 'G4', 'E4'],
      duration: '8n',
      interval: '8n'
    },
    effects: {
      reverb: { decay: 3.0, wet: 0.4 }
    },
    transport: { bpm: 120 },
    analysis: {
      fftSize: 1024,
      smoothing: 0.8,
      outputSmoothing: 0.15,
      beatThreshold: 0.15
    }
  };

  let gui;
  let time = 0;
  let audioInitialized = false;

  p.setup = () => {
    p.createCanvas(800, 800);
    p.colorMode(p.HSB, 360, 100, 100, 100);
    p.noStroke();
    loadConfig();
  };

  p.draw = () => {
    const audio = getAudioData();

    // Background: subtle darkening based on envelope
    const bgBrightness = 8 + audio.envelope * 5;
    p.background(config.bgHue, 20, bgBrightness);

    // Beat flash overlay
    if (audio.beat > 0.1) {
      p.fill(0, 0, 100, audio.beat * config.beatFlash * 100);
      p.rect(0, 0, p.width, p.height);
    }

    p.push();
    p.translate(p.width / 2, p.height / 2);

    // Rotation driven by treble
    const rotation = time * config.rotationSpeed + audio.treble * Math.PI;
    p.rotate(rotation);

    // Number of elements driven by treble detail
    const numElements = Math.floor(config.trebleDetail + audio.highMid * 12);

    // Size driven by bass
    const size = applyMapping(audio.bass, {
      min: config.baseSize * 0.5,
      max: config.baseSize + config.bassSizeScale,
      curve: 'cubicOut'
    });

    // Draw audio-reactive ring of circles
    for (let i = 0; i < numElements; i++) {
      const angle = (p.TWO_PI / numElements) * i;
      const radius = size + audio.mid * 50;

      const x = p.cos(angle) * radius;
      const y = p.sin(angle) * radius;

      // Hue shifts with mids
      const hue = (config.bgHue + config.midsHueShift * audio.mids + i * (360 / numElements)) % 360;
      const elementSize = 10 + audio.bass * 30 + p.sin(time + i) * 5;
      const alpha = 60 + audio.energy * 40;

      p.fill(hue, 70 + audio.treble * 30, 80 + audio.envelope * 20, alpha);
      p.circle(x, y, elementSize);
    }

    // Inner pulsing core
    const coreSize = 30 + audio.envelope * 60 + audio.bass * 40;
    const coreHue = (config.bgHue + 180 + audio.mids * config.midsHueShift) % 360;
    p.fill(coreHue, 60, 90, 80);
    p.circle(0, 0, coreSize);

    p.pop();

    time += 0.02;
  };

  async function loadConfig() {
    try {
      const response = await fetch('./config.json');
      const saved = await response.json();
      // Merge saved config (preserving nested objects)
      config = deepMerge(config, saved);
    } catch (e) {
      console.log('No saved config, using defaults');
    }

    // Initialize audio with config
    initAudio(config);
    audioInitialized = true;

    setupGUI();
    setupTransportButtons();
  }

  function setupTransportButtons() {
    const playBtn = document.getElementById('playBtn');
    const stopBtn = document.getElementById('stopBtn');

    if (playBtn) {
      playBtn.addEventListener('click', async () => {
        await startAudio(config.transport.bpm);
        playBtn.classList.add('active');
      });
    }

    if (stopBtn) {
      stopBtn.addEventListener('click', () => {
        stopAudio();
        if (playBtn) playBtn.classList.remove('active');
      });
    }
  }

  function setupGUI() {
    gui = new GUI({ title: '{{ATOM_NAME}}' });

    // Visual controls
    const visFolder = gui.addFolder('Visual');
    visFolder.add(config, 'bgHue', 0, 360).name('Background Hue');
    visFolder.add(config, 'baseSize', 30, 300).name('Base Size');
    visFolder.add(config, 'particleCount', 10, 100).step(1).name('Particles');
    visFolder.add(config, 'rotationSpeed', 0, 3).name('Rotation');

    // Audio-visual mapping controls
    const mapFolder = gui.addFolder('Audio Mapping');
    mapFolder.add(config, 'bassSizeScale', 0, 500).name('Bass -> Size');
    mapFolder.add(config, 'midsHueShift', 0, 360).name('Mids -> Hue');
    mapFolder.add(config, 'trebleDetail', 3, 24).step(1).name('Treble -> Detail');
    mapFolder.add(config, 'beatFlash', 0, 1).name('Beat Flash');

    // Transport
    const transportFolder = gui.addFolder('Transport');
    transportFolder.add(config.transport, 'bpm', 40, 200).step(1).name('BPM');

    gui.onChange(() => {
      console.log('Copy to config.json:', JSON.stringify(config, null, 2));
    });
  }

  function deepMerge(target, source) {
    const result = { ...target };
    for (const key of Object.keys(source)) {
      if (source[key] && typeof source[key] === 'object' && !Array.isArray(source[key])) {
        result[key] = deepMerge(target[key] || {}, source[key]);
      } else {
        result[key] = source[key];
      }
    }
    return result;
  }
};

p5Instance = new p5(sketch);

// Vite HMR cleanup
if (import.meta.hot) {
  import.meta.hot.dispose(async () => {
    await cleanupAudio();
    if (p5Instance) {
      p5Instance.remove();
      p5Instance = null;
    }
  });
}
```

**cli/templates/audio-visual/config.json:**

```json
{
  "type": "audio-visual",
  "bgHue": 240,
  "baseSize": 100,
  "particleCount": 60,
  "rotationSpeed": 0.5,
  "bassSizeScale": 200,
  "midsHueShift": 120,
  "trebleDetail": 8,
  "beatFlash": 0.8,
  "synth": {
    "type": "mono",
    "oscillator": { "type": "triangle" },
    "envelope": {
      "attack": 0.01,
      "decay": 0.3,
      "sustain": 0.4,
      "release": 0.8
    }
  },
  "sequence": {
    "notes": ["C4", "E4", "G4", "B4", "C5", null, "G4", "E4"],
    "duration": "8n",
    "interval": "8n"
  },
  "effects": {
    "reverb": {
      "decay": 3.0,
      "wet": 0.4
    }
  },
  "transport": {
    "bpm": 120
  },
  "analysis": {
    "fftSize": 1024,
    "smoothing": 0.8,
    "outputSmoothing": 0.15,
    "beatThreshold": 0.15
  }
}
```

**cli/templates/audio-visual/NOTES.md:**

```markdown
# {{ATOM_NAME}}

**Created:** {{DATE}}
**Stage:** idea
**Type:** audio-visual

## Intent
What audio-visual relationship am I exploring? What should the viewer feel?

## Audio Design
- Synth type:
- Key/scale:
- Tempo:
- Effects:

## Visual Design
- What shapes/patterns?
- Color palette:
- Movement style:

## Audio-Visual Mapping
- Bass drives:
- Mids drive:
- Treble drives:
- Beat triggers:

## Technical Decisions
- Why these mappings?
- Smoothing/easing choices:

## Session Log

### {{DATE}} {{TIME}}
- Created audio-visual atom
- Initial setup
```

**Update cli/commands/create.js** -- Add `audio-visual` to valid types:

Update the valid types array to include `audio-visual`:
```javascript
const validTypes = ['visual', 'audio', 'audio-visual'];
```

The template path pattern `path.join(__dirname, '../templates/${type}')` already handles this since the folder is named `audio-visual`.
  </action>
  <verify>
1. `eoe create audio-visual reactive-demo` creates audio-visual atom folder
2. All template files present: index.html, sketch.js, audio.js, config.json, NOTES.md
3. No {{ATOM_NAME}} placeholders remaining in any file
4. `eoe dev <audio-visual-atom>` starts Vite and opens the combined atom
5. Canvas shows animated p5.js sketch (ring of circles)
6. Click Play -- audio starts, visuals begin reacting:
   - Circles pulse larger on bass hits
   - Colors shift with mid-frequency content
   - Detail increases with treble content
   - Flash on beat detection
7. Click Stop -- audio stops, visuals return to quiet state
8. lil-gui shows Visual, Audio Mapping, and Transport sections
9. Adjusting "Bass -> Size" in GUI changes how dramatically bass affects circle size
10. HMR cleanup: edit sketch.js while playing -- audio stops cleanly, canvas resets, re-initializes
  </verify>
  <done>
Audio-visual atom template created with combined Tone.js audio and p5.js reactive sketch. Demo shows ring of circles reacting to audio: bass drives size, mids shift color, treble adds detail, beats trigger flash. Separate audio.js module exports getAudioData() for clean separation. CLI extended to support `audio-visual` type. lil-gui provides both visual parameters and audio-visual mapping controls.
  </done>
</task>

<task type="auto">
  <name>Task 4: Verify audio analysis accuracy and visual reactivity</name>
  <files>
    (no new files -- verification and tuning task)
  </files>
  <action>
Test the audio analysis pipeline end-to-end with the audio-visual demo to verify all metrics produce meaningful values and the visual response feels natural.

**Analysis Accuracy Tests:**
1. Create an audio-visual test atom: `eoe create audio-visual analysis-test`
2. Start dev server and play audio
3. Open browser console and add logging:
   ```javascript
   // In the browser console while the atom is running
   setInterval(() => {
     const data = window.__audioData; // or inspect from sketch
     console.table({
       bass: data?.bass?.toFixed(3),
       mids: data?.mids?.toFixed(3),
       treble: data?.treble?.toFixed(3),
       beat: data?.beat?.toFixed(3),
       envelope: data?.envelope?.toFixed(3)
     });
   }, 500);
   ```
4. Verify:
   - bass value responds to low-frequency content (higher for bass-heavy parts)
   - mids value responds to mid-range (melody notes)
   - treble responds to high-frequency content
   - beat fires on rhythmic accents (beat value spikes to ~1.0 then decays)
   - envelope tracks overall volume smoothly

**Smoothing Quality Tests:**
5. Verify smoothing prevents jitter:
   - Visual elements should move smoothly, not jump erratically
   - Size changes should feel like breathing, not flickering
   - Color shifts should transition smoothly
6. Verify beat detection:
   - Flash should fire at perceptible rhythm points
   - No false triggers during sustained notes
   - Beat value decays naturally between beats

**Easing Function Tests:**
7. Test different easing curves in config:
   - Change bass mapping to 'logarithmic' -- verify more sensitive to quiet bass
   - Change to 'exponentialOut' -- verify dramatic response to loud bass
   - Verify linear is the default and works as expected

**Performance Test:**
8. Monitor frame rate:
   - Open Chrome DevTools Performance tab
   - Record 10 seconds of audio-visual playback
   - Frame rate should stay above 55fps
   - No audio glitches or visual stutters

If frequency band values seem off (too high/low), tune the minDb/maxDb parameters in analyser.js or the bin ranges in bands.js. If beat detection is too sensitive or insensitive, adjust the threshold in config.json.

Clean up test atom after verification:
```bash
rm -rf atoms/YYYY-MM-DD-analysis-test
```
  </action>
  <verify>
1. All frequency bands produce values in 0-1 range during playback
2. Band values correspond to expected frequency content
3. Beat detection fires at musically relevant moments
4. Envelope follows volume contour smoothly
5. Visual response feels natural and "reactive" (not "controlled")
6. Smoothing prevents jitter without adding excessive lag
7. Easing functions produce perceptibly different responses
8. Frame rate stays above 55fps during audio-visual playback
9. No console errors during normal operation
  </verify>
  <done>
Audio analysis pipeline verified end-to-end: frequency bands accurately reflect spectral content, beat detection fires at rhythmic accents, envelope follows volume smoothly, easing functions produce correct nonlinear mappings. Visual response feels natural and responsive. Performance within acceptable bounds (>55fps).
  </done>
</task>

</tasks>

<verification>
1. Audio analysis modules all functional: analyser, bands, beat detection, envelope, smoothing
2. AudioDataProvider aggregates all metrics into single per-frame object
3. Frequency bands (bass, lowMid, mid, highMid, treble) produce 0-1 values
4. Beat detection fires on rhythmic accents with natural decay
5. Envelope follower tracks volume with attack/release smoothing
6. Easing functions map 0-1 values through nonlinear curves
7. Audio-visual template demonstrates reactive sketch
8. `eoe create audio-visual <name>` scaffolds combined atom
9. Visual response to audio feels natural ("reacting" not "controlled")
10. Performance >55fps during audio-visual playback
</verification>

<success_criteria>
- Tone.Analyser produces normalized 0-1 frequency data (AUD-03 foundation)
- Frequency bands map to perceptually relevant ranges (bass: 60-250Hz, mid: 500-2kHz, treble: 4-8kHz)
- Beat detection using spectral flux with adaptive threshold
- Envelope follower with attack/release smoothing and auto-gain normalization
- Exponential smoothing applied by default (CONTEXT.md decision: "prevent jitter")
- 7 easing functions available for nonlinear mapping (CONTEXT.md: "exponential, logarithmic, sine")
- AudioDataProvider class aggregates all analysis into { bass, mid, treble, beat, envelope, ... }
- Audio-visual atom template shows immediately compelling reactive visuals
- Bass controls size, mids control color, treble controls detail, beats trigger flash
- getAudioData() provides clean API for sketch consumption
- Frame rate stays above 55fps with analysis running
</success_criteria>

<output>
After completion, create `.planning/phases/02-audio-integration/02-02-SUMMARY.md`
</output>
